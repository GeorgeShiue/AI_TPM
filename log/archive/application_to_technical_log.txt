---------- TextMessage (user) ----------

以下是提供你閱讀的計畫文件：
===
附件四、報告格式 
 
 
 
 
 
 
即時海洋觀測平台開發暨AI影像數據應用服
務主題式研發計畫 
 
期末報告1 
 
即時海洋觀測平台開發暨 AI 影像數據應用服務主題式研發
計畫 
 
 
 
 
期末報告 
 
 
 
 
 
 
 
 
計畫主持人：林家瑜 助理教授     
研究機構  ：國立中央大學資訊工程系 
執行期間  ：113/05/01-114/04/30（請填寫合作研究計畫執行期間） 
 
(以上資料請務必以中文繕打清楚) 
 
中華民國 114 年 01 月31 日2 
 
計畫摘要 
    本研究致力於開發即時海洋觀測平台，並應用 AI 影像數據分析來提升遠距離船
隻辨識的準確性與效率。現行船舶識別技術主要依賴人眼辨識，然而在 3 公里以上距
離，由於海面反射、霧氣與光線變化等環境因素的影響，傳統方法的準確率明顯下
降。本計畫旨在透過深度學習物件辨識技術，建立一套高效能的「即時遠距離多船隻
辨識系統」，能夠在 3 公里外準確偵測船隻，並可同時識別 10 艘以上長度超過 15 公
尺的船隻。 
    本研究參考 YOLOv5s 與YOLOv8 物件辨識模型，針對海洋環境特性進行改良，
並利用多種公開及自建數據集如 Singapore Maritime Dataset、ABOships-PLUS、高雄
港實測數據進行模型訓練與驗證。透過影像增強、特徵提取優化與多尺度檢測技術，
我們顯著提升了模型在複雜海洋場景中的識別能力。 
    目前的實驗結果顯示，我們的系統在新加坡與高雄測試數據集中的部分影像皆展
現超過 90%的識別準確度，並成功達成： 
⚫ 可偵測移動速度超過 15 節的船隻 
⚫ 在 5 公里範圍內實現高效識別 
⚫ 可即時處理 640×480 解析度影像，幀率≥10FPS 
⚫ 具備船隻避碰預警功能 
    未來將進一步提升船隻辨識與船隻避碰預測的準確度，並擴充模型至夜間與黑白
影像辨識，以提升即時監測與應用能力。此技術的發展對於海洋交通安全監控、智慧
港口管理與海上救援應用將具備重要價值。 
 
關鍵字：即時海洋觀測、AI 影像辨識、深度學習、YOLOv5、YOLOv8、多尺度檢
測、船隻辨識、避碰預警、智慧港口、遠距離監測3 
 
目錄 
計畫摘要 ....................................................................................................................... 2 
一、計畫動機與目的 ................................................................................................... 4 
1. 動機 ..................................................................................................................... 4 
2. 目的 ..................................................................................................................... 4 
二、文獻探討 ............................................................................................................... 5 
三、研究方法 ............................................................................................................... 8 
1. 資料收集 ............................................................................................................. 8 
2. 資料前處理 ....................................................................................................... 10 
3. 船隻辨識模型 ................................................................................................... 10 
四、實驗結果 ............................................................................................................. 12 
五、結論 ..................................................................................................................... 18 
六、參考文獻 ............................................................................................................. 194 
 
一、 計畫動機與目的 
1. 動機 
在海洋環境中，對於船隻的及時且準確的辨識是保障航行安全和有效執
行海上作業的關鍵。但目前大部分的船隻都是人工辨識，而人眼無法同時看
超過 10 艘船隻進行辨識，因此需要 AI 輔助。現有的 AI 對距離近且較大的
船或是俯視角的衛星雷達成像的船有不錯的辨識能力，但若是在第一人稱且
距離超過 3 公里的情況下，可能會受到環境條件如海面反射、霧氣、光線變
化等的影響，導致偵測的準確度降低，且現有開放資料集大多是較大的船隻
，3 公里以上的船隻資料集較少，不利於訓練模型。 
2. 目的 
基於上述的問題，本計畫欲設計一套「即時遠距離多船隻辨識系統」 ，
此系統可以有效辨識 3 公里遠的船隻，且一次可辨識長度15 公尺的船隻 10
艘以上。5 
 
二、 文獻探討 
現在有一些研究採用物件辨識模型進行船隻辨識，在 Automatic Ship 
Object Detection Model Based on YOLOv4 with Transformer Mechanism in 
Remote Sensing Images 論文中提出了名為 Auto-T-YOLO 的架構[1]，用於解
決現有物件偵測模型從輸入中提取特徵未能充分考慮全局特徵，且無法根據
輸入的特徵自動調整，導致偵測準確度下降的問題。 
    Auto-T-YOLO 的模型架構分為三個部分 ，Preattention, Attention 和
Prediction。輸入影像大小為608×608×3，Preattention 和 Attention 的設計
模仿了人類視覺系統中從大的全局特徵開始，再逐步轉向局部細節的兩階段
注意機制。 
    在 Preattention 架構中，輸入的圖像會根據其中是否包含𝑠𝑖𝑧𝑒 > 32×32
的物件分為 A 子資料集及 B 子資料集，並在之後的 Attention 架構中根據其
所在的子資料集做不同的影像處理。 
    Attention 架構主要參考了 YOLOv4 的 Backbone 及 Neck 架構來設計，
A 子資料集及 B 子資料集在 Backbone 中的處理流程相同，都是利用一層
CBM 進行卷積運算以及五層 CSPDarknet53 進行殘差及卷積運算來提取特
徵同時防止梯度消失，由於YOLOv4 的 CBL 架構基於卷積運算，而卷積運
算受限於 Kernel 大小，因此無法很好地提取全局特徵，所以 A 子資料集在
Neck 中 SPP 架構輸出之後使用了 Transformer 中的 Multi-head Self-Attention
架構取代了 CBL，稱為 SPP-MHSA，其中 Multi-head Self-Attention 使用了
四個 head。Neck 的後半段與 YOLOv4 相似，使用了 FPN 做 Upsampling、
PANet 做 downsampling 以提升多尺度目標檢測的能力。 
    Prediction 架構用來對 PANet 輸出做多尺度目標檢測，損失函數使用了
CIoU 以及 DIoU-NMS。模型準確度在 SSDD 資料集、近海及遠海場景上分
別達到了 96.3%、91.78%以及 98.33%。 
 在另一篇論文 A streamlined approach for intelligent ship object detection 
using EL -YOLO algorithm 中提出了 EL-YOLO 的架構[2]，用於提升目標檢
測在具有波浪、反射等複雜海洋環境中對於小物體的檢測精度，其輕量化的
網路架構在硬體運算資源有限的智慧船舶、無人船舶等領域也有良好的適應
性。 
EL-YOLO 的架構基於 YOLOv8 進行改進，輸入影像大小為640×640
，包含了Backbone、Neck、Head 三個部分，Backbone 中與 YOLOv8 相同，
使用了一層CBS 進行卷積運算和四層CBS 加C2f 進行殘差及卷積運算來提
取特徵同時防止梯度消失。 
在 Neck 部分，因為傳統的 FPN 和 PAN 架構在面對因反射或波浪等背
景干擾因素導致特徵信息不足的目標時，容易在多層卷積過程中丟失信息，
導致檢測精度下降，所以作者提出了 SMFN (short multi‑fuse neck )架構，在6 
 
YOLOv8 的基礎上，額外加入了較大尺寸的兩個輸入 (320×320和160×
160)、一個 Upsampling 層(160×160) 、一個 Fusion 加 C2f 層(160×160)
、一個卷積層(80×80)及一個輸出(160×160)到 FPN 和 PAN 架構中，以
捕捉更多的特徵信息，並提高對小型目標的檢測能力。 
Head 架構因為 Neck 的 SMFN 架構而多了一個160×160的輸出，相較
原本只有三個輸出的 YOLOv8 多了一個。在損失函數的部分，因為傳統的
CIoU 在處理特徵信息不足的目標時 往往會過度懲罰， 因此作者提出了
AWIoU(adequate wise IoU )，通過動態分配不同權重給不同特徵信息量的目
標來調整懲罰，避免目標因受背景干擾而造成模型誤判。 
在模型訓練完後，為了使其網路架構輕量化以應用於硬體運算資源有限
的智慧船舶 、 無人船舶等領域，作者提出了GDFP(Greedy-driven filter pruning)
，首先計算每一層中所有 filter 的 L1 範數以得到其重要性分數，並將分數最
低的 filter 刪除，在刪除後對模型重新評估，並重複以上操作直到得到理想
的模型，最後重新訓練該模型，以得到將模型輕量化的效果。 
此模型在 SeaShips 資料集上的mAP0.5 為 0.988， mAP0.5:0.95 為 0.778
，在 ABOships 資料集上的mAP0.5 為 0.672，mAP0.5:0.95 為 0.348。 
另一篇論文  YOLOSeaShip: a lightweight model for real -time ship 
detection [3]中提出了 YOLOSeaShip 的架構，用於解決海洋環境中目標檢測
的準確性與硬體資源需求之間的矛盾，特別是在實時性要求較高且硬體運算
能力有限的應用場景如邊境監控、智慧海洋交通中。 
    YOLOSeaShip 的架構基於 YOLOv7-tiny 進行改進，包含了 Input、
Backbone 和 Head 三個主要部分。 
Input 架構上採用了 Mosaic 數據增強技術，通過隨機裁剪並拼接多張影
像，增加數據的多樣性與小目標樣本數。此外，使用自適應圖片縮放技術來
處理不同尺度的目標，並結合K-means 分群方法優化錨框的尺寸設定，避免
人工設計過程中的偏差。 
Backbone 的部分，YOLOSeaShip 引入了 PConv（Partial Convolution ）
架構替代了原本的卷積操作。該架構僅對部分通道進行計算，從而有效減少
了參數量並提升計算效率。此外，每層池化操作後 還加入了 PfAAM（
Parameter-Free Average Attention Module）。PfAAM 基於空間和通道的平均值
計算注意力權重，不增加模型參數的情況下提升了模型對關鍵區域的定位能
力。 
Head 部分，模型加入了SPPCSPC_tiny 架構，通過多尺度（5×5、9×9 和
13×13）最大池化層結合本地與全局特徵，增強對不同大小目標的檢測能力
。模型的最終輸出包含三個不同大小的特徵圖，用於處理不同尺度的目標檢
測。 
在損失函數設計上，作者提出了 Focal EIOU（Focal and Efficient IOU）
損失函數，結合了 EIOU 和 Focal Loss 的優點。EIOU 損失通過細化目標框7 
 
的寬高比差異來提升回歸精度，而Focal Loss 則用於平衡正負樣本比例，讓
模型更關注高質量的預測框。 
為了評估模型性能，作者在 SeaShips(7000)資料集上進行實驗，訓練時
採用 Adam 優化器，學習率為 0.001，訓練 300 個 epoch。結果顯示，
YOLOSeaShip 的 mAP 達到 0.976，檢測速度達到119.84 FPS，相較YOLOv7-
tiny 模型，參數量減少至 4.78M，同時大幅提升了檢測速度和資源效率。 
在論文 An Improved YOLO v4 Algorithm-based Object Detection Method 
for Maritime Vessels[4]中提出了基於改進 YOLOv4 的船舶和泊位檢測方法，
用於提升目標檢測在複雜海洋環境下的準確性，特別是對港口泊位的檢測效
果進行了優化。該方法通過改良 錨框的生成方式和數據集構建，改善了
YOLOv4 在海洋場景中的檢測性能，適用於智能船舶的安全輔助系統。 
改進的 YOLOv4 架構包含了 Backbone、Neck 和 Head 三個部分。
Backbone 部分採用了 CSPDarknet53 作為特徵提取模塊，通過 ResUnit 組件
進行深度卷積操作，增強特徵提取能力。Neck 部分引入了 SPP 層和 PANet
層，其中 SPP 層通過多尺度池化操作擴展感受野，而 PANet 則將底層的定
位信息與高層的語義信息結合，提高多尺度目標的檢測能力。Head 部分則
採用了 YOLOv3 的檢測頭，輸出不同尺度的檢測結果。 
針對泊位檢測效果不足的問題，作者採用了 K-means 演算法重新生成
錨框。在原始 YOLOv4 模型中，錨框基於 MS COCO 和 VOC 數據集生成，
但這些框與船舶與泊位數據集中的物體尺寸不匹配，影響了檢測精度。改進
後的 K-means 演算法針對數據集進行分群，生成了9 組適配的錨框，有效提
升了檢測效率和準確性。 
此外，作者構建了一個包含船舶和泊位的專用數據集，通過智能實驗船
上的攝像頭在港口拍攝的實景影像生成，並結合公開數據集如MS COCO 和
VOC 補充正樣本數量。最終數據集中包含6634 張標註影像，其中70%用於
訓練，30%用於測試。標註信息包括物體類別及邊界框的坐標和尺寸。 
實驗結果表明，改進的YOLOv4 模型在 mAP 和 F1-score 方面分別提升
了 2.79%和 0.80%，相較於原始YOLOv4 達到了更高的檢測精度與穩定性。
具體指標顯示，改進後的YOLOv4 模型在 mAP 上達到 83.98%，F1-score 達
到 86.18%，而 YOLOv4_tiny 僅達到 54.45%的 mAP 和 54.82%的 F1-score，
說明輕量化模型雖然具有更快的檢測速度，但在檢測準確性上仍有進步空間
。8 
 
三、 研究方法 
1. 資料收集 
    資料集使用 Singapore Maritime Dataset 的 Visible On-Shore 資料集[5, 6]加上
ABOships-PLUS[7]以及手動標註公司提供的高雄資料集。 
    Visible On-Shore 資料集為網路上的公開資料集，包含 38 部長度小於 33 秒
的全彩影片，屬於中、遠港船隻影像資料集，其中含有 7 個船隻類別，此資料集
特徵為船隻背景為海面、天空等較乾淨的畫面。 
 
表一、Visible On-Shore 資料集類別 
Lable Data   
Ferry 
 
SailsBoat 
 
PatrolBoat 
 
MerchantShip 
 
FishingBoat 
 
Tugboat 
 
BargeShip9 
 
    ABOships-PLUS 資料集為網路上的公開資料集，ABOships 資料集的改進版
本，包含 9880 張影像，33227 個標註，屬於近港船隻影像資料集，其中含有 3 個
船隻類別，此資料集特徵為船隻背景包含陸地建築物等容易誤判的畫面。 
 
表二、ABOships-PLUS 資料集類別 
Lable Data   
powerboat 
 
sailboat 
 
ship 
 
  
 
    高雄資料集為高雄實地拍攝的非公開資料集，包含 504 部長度小於 10 秒的
影片，屬於近、中港船隻影像資料集，其中含有 4 個船隻類別，此資料集特徵為
船隻背景包含陸地建築物等容易誤判的畫面。 
 
表三、高雄資料集類別 
Lable Data   
PatrolBoat 
 
Yacht 
 
MerchantShip 
 
Tugboat10 
 
2. 資料前處理 
    
 
圖一、資料前處理流程圖。 
    如圖一所示，首先會將資料集處理成只包含 images 和 labels 資料夾的
形式，images 和 labels 中的檔案是兩兩對應的，並且labels 內的標註檔格式
為 yolo 格式。再來會刪除有問題的標註檔及圖片，例如標註檔為空、包含
不存在類別、格式錯誤、圖片模糊或者有破圖情況等。第三步是合併資料集
，這步是可選的，主要會將數個資料集合併，並記錄合併後的類別名稱。第
四步是篩選類別，這步同樣為可選，主要會將需要的類別保留，不需要的剔
除。接著因為同一圖片中可能包含數量不等的不同類別物件，因此在切分訓
練集和驗證集時，容易造成不同類別的訓練驗證比例不同的情況，所以第五
步會將包含 n 個物件的標註檔拆分成 n 份，每一份只包含1 個原始標註檔的
物件，彼此不重複，且與其對應的圖片同樣複製 n 份，以確保每個類別都按
照正確比例進行切分，最後一步為按照自訂比例切分訓練集及驗證集。 
3. 船隻辨識模型 
    我們使用了最新的YOLOv11[8]模型來建置船隻辨識模型 ，YOLOv11 基
於 PyTorch 架構開發，可在一次前向傳播中同時定位及辨識物件，具有速度
快、所需資源少、開發便利及多尺度檢測等優點。圖二為 YOLOv11 架構圖
，首先輸入的圖片為 640x640x3 的 RGB 圖像，接著 YOLOv11 可以根據此
架構分為三個部分，Backbone, Neck 以及 Head。11 
 
 
圖二、YOLOv11 架構圖[9] 
 
Backbone 如圖二左側所示，主要目的是從輸入影像中提取有用的特徵
資訊。在 YOLOv11 中，Backbone 採用了 C3K2 結構，為升級版的 C3，透
過更小的卷積核與路徑切分技術，在保持模型表現的前提下大幅降低了計算
成本。每個C3K2 中包含兩條路徑，一條直接進行卷積處理，另一條則經過
多個 Bottleneck 區塊進行深度特徵學習，最後進行特徵融合。這種結構不僅
有效防止梯度消失問題，也能提升網路的收斂速度與特徵提取效率。 
Neck 部分如圖二中間所示，結合了 SPFF(Spatial Pyramid Pooling Fast)
架構和多層 C2PSA 區塊。SPFF 架構是 SPP 的高速版本，透過不同尺度的
最大池化操作將特徵圖進行多尺度融合，再透過 Concatenation 將資訊彙整
，幫助模型更好地識別大小不一的船隻。此外，Neck 中的 C2PSA 架構引入
空間注意力機制(Spatial Attention)，能讓模型聚焦在關鍵影像區域，特別是
對小型或部分遮蔽的物件表現更佳。這部分的架構延續了FPN 與 PAN 的優
點，強化了從深層到淺層、再回到深層的資訊傳遞，進一步提升辨識與定位
的能力。 
Head 如圖二右側所示，負責進行最後的物件偵測任務。YOLOv11 採用
多尺度輸出策略，透過多層輸出特徵圖分別負責不同大小目標的偵測。
YOLOv11 會利用不同尺寸與比例的錨框(Anchor Boxes)來涵蓋不同形狀的
目標，並透過 Non-Maximum Suppression(NMS)過濾重疊的邊界框，保留最
具代表性的偵測結果。12 
 
四、 實驗結果 
    我們使用 YOLOv11l 模型，訓練集使用 ABOships-PLUS，測試集使用
高雄資料集，實驗環境為 64-bit Windows 11、Intel i5-13400F 2.5GHz CPU、
32GB RAM 及 NVIDIA RTX 4070 GPU。除了測試模型在原先新加坡資料集
的效果外，實驗還在高雄資料集上針對船隻速度、距離以及避碰能力做檢測
，因實際場域在白天時使用的是彩色影像，黑夜時使用的是黑白影像當作模
型輸入，所以實驗也同時測試了模型對彩色和黑白影像的辨識能力，檢測方
式為當船隻出現的 3 秒內有辨識到至少 1 次則辨識成功，反之辨識失敗，同
一部影片中辨識成功的數量與真實船隻的數量比值即為檢測率。實驗時我們
同時比較了 1 秒內有辨識到船隻、2 秒內有辨識到船隻的檢測率以更精確地
測量模型的辨識能力，以下為檢測效果。 
 
表四、實驗結果檢測率 
測試影片 1 秒內檢測率 2 秒內檢測率 3 秒內檢測率 
彩色影片 output_005 1.0 1.0 1.0 
彩色影片 output_041 1.0 1.0 1.0 
彩色影片 output_080 1.0 1.0 1.0 
彩色影片 output_097 1.0 1.0 1.0 
黑白影片 output_199 0.67 1.0 1.0 
黑白影片 output_202 0.67 1.0 1.0 
黑白影片 output_276 1.0 1.0 1.0 
黑白影片 output_290 0.75 0.75 1.0 
 
    如上表所示，原要求的 3 秒內檢測率全部達標，都為 1.0，2 秒與 1 秒
內的檢測率也大部分都是 1.0，其中判錯的情況都是在船隻彼此重疊時或影
像較模糊時發生，一般的應用情況幾乎不會有問題。13 
 
 
圖三、彩色影片 output_005 第一秒影像 
    彩色影片 output_005 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。 
 
圖四、彩色影片 output_041 第一秒影像 
    彩色影片 output_041 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。14 
 
 
圖五、彩色影片 output_080 第一秒影像 
    彩色影片 output_080 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。 
 
圖六、彩色影片 output_097 第一秒影像 
    彩色影片 output_097 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。15 
 
 
圖七、黑白影片 output_199 第二秒影像 
    彩色影片 output_199 的第二秒影像如上圖所示，圖中模型成功辨識出所
有船隻，2~3 秒內的檢測率都為 1.0。 
 
圖八、黑白影片 output_202 第二秒影像 
    彩色影片 output_202 的第二秒影像如上圖所示，圖中模型成功辨識出所
有船隻，2~3 秒內的檢測率都為 1.0。16 
 
 
圖九、黑白影片 output_276 第一秒影像 
    彩色影片 output_276 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。 
 
圖十、黑白影片 output_290 第三秒影像 
    彩色影片 output_290 的第三秒影像如上圖所示，圖中模型成功辨識出所
有船隻，3 秒內的檢測率都為 1.0。17 
 
表五、KPI 完成表 
項目 達成狀況 
1.建立 AI 分析海事觀測演算法邏輯架構 已達成 
2.完成演算法建立，並可在本島沿海 3 公里海域內環境進行 已達成 
3.演算結果可達以下規格標準: 
通訊傳輸速率>250kbps 以上 已達成 
影像解析度> 640x480 已達成 
影像幀速率≧ 10FPS 已達成 
船隻移動速度≧15 節(30km/hr) 已達成 
船隻辨識數量≧ 10 已達成 
船隻辨識大小> 15m 已達成 
船隻辨識距離≧5km 已達成 
船隻避碰距離≧3.7km 已達成 
船隻辨識準確度≧90% 已達成 
 
KPI 完成表如上所示，全部項目都已完成。18 
 
五、 結論 
本計畫目的為設計出可以有效辨識 3 公里遠的船隻，且一次可辨識長度 15
公尺的船隻 10 艘以上的模型，根據目前的實驗結果，模型在船隻距離、速度、
目標數量以及避碰能力上都有達到目標，並且模型在彩色及黑白影片上的表現也
能夠符合實際應用場域需求。19 
 
六、 參考文獻 
[1] Sun, Bowen, et al. "Automatic ship object detection model based on YOLOv4 
with transformer mechanism in remote sensing images." Applied Sciences vol. 
13. No. 4, p. 2488, 2023. 
[2] Yang, Defu, et al. "A streamlined approach for intelligent ship object detection 
using EL-YOLO algorithm." Scientific Reports, vol. 14. No.1, p.15254, 2024. 
[3] Jiang, X., Cai, J., & Wang, B. (2024). YOLOSeaShip: a lightweight model for 
real-time ship detection. European Journal of Remote Sensing, 57(1). 
https://doi.org/10.1080/22797254.2024.2307613 
[4] He, Guowen & Wang, Wenlong & Shi, Bowen & Liu, Shijie & Xiang, Hui & 
Wang, Xiaoyuan. (2022). An Improved YOLO v4 Algorithm-based Object 
Detection Method for Maritime Vessels. International Journal of Science and 
Engineering Applications. 11. 50-55. 10.7753/IJSEA1104.1001. 
[5] D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabaly, and C. Quek, “Video 
Processing from Electro-optical Sensors for Object Detection and Tracking in 
Maritime Environment: A Survey,” IEEE Transactions on Intelligent 
Transportation Systems (IEEE), 18 (8), 1993 - 2016, 2017. 
[6] D. K. Prasad, “Singapore Maritime Dataset, Visible On-Shore dataset,” Available 
Online: https://sites.google.com/site/dilipprasad/home/singapore-maritime-
dataset 
[7] Winsten Jesper, Iancu Bogdan, Soloviev Valentin, & Lilius Johan. (2024). 
ABOships-PLUS [Data set]. Zenodo. https://doi.org/10.5281/zenodo.10469672 
[8] Glenn Jocher and Jing Qiu, “Ultralytics YOLO11,” Available Online: 
https://github.com/ultralytics/ultralytics 
[9] Medium, “YOLOv11 Architecture Explained: Next-Level Object Detection with 
Enhanced Speed and Accuracy,” Available Online: https://medium.com/@nikhil-
rao-20/yolov11-explained-next-level-object-detection-with-enhanced-speed-and-
accuracy-2dbe2d376f71
===
簡單整理目前的想像畫面：
- 使用者是岸邊的監控中心操作人員，負責24小時不間斷監控約24海里（約44公里）範圍內海域的船隻動向。
- 系統需要持續自動掃描，並能在船隻進入或快速靠近關鍵區域時，及時在螢幕上標示其位置並發出警示，協助管控及調度決策。
- 在距離較近的地方，掃描頻率及更新速度要高，確保船隻動態能即時反映；而遠距離船隻則可以較低頻率掃描，平衡系統負載與效率。
- 影像辨識必須涵蓋夜間及低光環境，對黑白影像或雨霧等惡劣天氣需有一定的精準度，避免錯過重要目標。

基於這個使用情境，遠距離即時辨識的主要挑戰可能會包含：
1. **影像品質與解析度**：遠距離的船隻在影像中尺寸會非常小，光線及水氣環境易降低辨識率。
2. **傳輸與處理效率**：解析高解析度影像需要較大頻寬與計算資源，尤其是24海里範圍涵蓋面積廣。
3. **環境干擾**：反射、波浪、霧氣等變數更顯著，會影響模型的判斷準確度。
4. **多層次巡檢策略**：需針對不同距離船隻設定合理的掃描頻率及預警門檻，避免系統過載且保持監控效能。
5. **低光源與夜間辨識**：必須提升模型對黑白或低光影像的準確率，可能需導入專門的影像增強或特殊感測器。

---------- TextMessage (TechnicalAgent) ----------
根據提供的計畫文件與使用情境描述，以下提出兩種可行的技術組合方案，針對遠距離即時海洋監控與AI影像辨識的需求：

---

## 方案一：多階段影像分級辨識系統（分層掃描+多模型融合）

### 適用條件

- 監控範圍廣（24海里約44公里），需兼顧即時性與系統負載平衡
- 高頻率近距離掃描與低頻率遠距離掃描混合需求
- 環境條件變化大（霧氣、光線波動、夜間低光）
- 需要較強的擴展性以因應未來多來源資料融合（如雷達、AIS數據等）

### 核心模組與系統層級流程

1. **影像資料采集模組**
    - 多組不同焦段與解析度的攝影機（如固定長焦鏡頭+廣角鏡頭）
    - 可搭配紅外線或熱成像以補足夜間及惡劣天氣的影像品質不足

2. **分層掃描排程模組**
    - 根據距離分為近距離（<5公里）、中距離（5-15公里）、遠距離（>15公里）三層
    - 近距離區域實施高頻率多角度掃描，遠距離區域則採用低頻率掃描策略，減少計算與頻寬負擔

3. **多模型融合辨識模組**
    - 近距離採用輕量化高速模型（例如方案文件中的 YOLOSeaShip 或 YOLOv11 Tiny 版本）進行快速辨識
    - 遠距離及黑白影像使用經過訓練的深度學習模型（如改良版 YOLOv11 搭配多尺度檢測和空間注意力機制）
    - 利用後端融合器（Ensemble Mechanism）整合不同模型輸出，提升整體識別準確率及穩定性

4. **影像增強與預處理模組**
    - 包含去霧、去反光、影像超分辨率重建等技術協助提升極端環境下的影像質量
    - 夜間黑白影像利用專門的圖像強化演算法進行處理增加辨識效果

5. **事件偵測與預警模組**
    - 建立基於空間位置與速度的船隻動態追蹤系統（融合Kalman Filter或Deep SORT等演算法）
    - 設定多重預警規則（如船隻進入特定區域、速度異常等），並透過即時視覺界面與告警系統通報操作人員

6. **後端服務與數據管理**
    - 高擴展性的雲端或邊緣計算平台部署
    - 支援海量影像資料存取、模型線上更新與訓練數據管理

### 擴展性與彈性

- 支援新增其他影像感測器（紅外線、雷達）或融合AIS等多元數據源，提升系統穩定性和警示誤報率降低
- 採取模組化架構便於後續替換或升級辨識模型
- 動態調整掃描頻率與影像解析度，依運算能力及網路狀態彈性調整

### 潛在風險與限制

- 高頻率掃描及多模型計算帶來較大運算與網路負擔，須做好硬體資源規劃
- 複雜模組間協調與同步難度高，尤其是在多模型結果融合與事件觸發邏輯設計
- 黑白及惡劣天氣影像辨識仍面臨準確率下降問題，需持續優化影像增強及模型訓練
- 資安面需嚴格管控影像資料傳輸與存取權限，防止敏感資料外洩

---

## 方案二：端側輕量化即時辨識+中央集中式智慧決策平台

### 適用條件

- 海域監控點分散，現場設置運算能力有限的裝置
- 需降低即時影像傳輸量以節省通訊頻寬與延遲
- 具備集中彙整多點資料進行高階分析與調度的需求

### 核心模組與系統層級流程

1. **前端端側辨識裝置**
    - 在各岸邊或浮標配備輕量化GPU或AI加速模組（如NVIDIA Jetson系列）
    - 部署輕量化、速度優先的模型（例如方案文件中提及的 YOLOSeaShip 或剪裁版 YOLOv11）
    - 端側即時處理採集的影像，執行目標檢測與追蹤，先行過濾與標定船隻訊息（位置、速度、類別）

2. **資料壓縮與傳輸模組**
    - 僅將辨識後的船隻資料（坐標、速度、警示狀態）及關鍵影像片段上傳至中央平台，減少頻寬需求
    - 傳輸使用安全加密通道，保護資料隱私與完整性

3. **中央智慧決策平台**
    - 負責整合多個端側裝置匯入的資訊，進行多點船隻動態整合與碰撞預測
    - 利用更大型的深度學習模型或統計模型，執行複雜預警分析與資源調度建議
    - 操作介面顯示整合後的海域監控地圖與警報通知

4. **夜間及惡劣氣象補充機制**
    - 中央平台協調前端感測器切換或補充（熱成像或雷達資料混合）
    - 可在特殊情況下遠端調用更高階模型進行重檢驗或調整端側模型參數

### 擴展性與彈性

- 易於分布式擴充，多個端點可逐步增加部署維護
- 中央平台可以加入AI模型持續更新，端側裝置亦可透過遠端下發模型更新
- 結構適合有多廠商或多系統接入，利於系統整合及擴展

### 潛在風險與限制

- 端側裝置硬體性能受限，可能限制辨識模型的精度及複雜度
- 中央與多端點間的網路連線穩定性影響系統整體運作與即時性
- 端側影像品質提升受限（硬體感測器限制、影像增強能力較弱）
- 系統整合複雜度高，需要嚴格設計通訊協議與容錯機制
- 資安風險增加，需重視多點資料同步安全與權限控管措施

---

以上兩方案皆建立於計畫中深度學習模型及資料集基礎，並兼顧使用者需求與系統實際運作條件，具備不同策略側重點：

- 方案一強調後端集中深度運算與多模態融合，適合硬體資源豐富與統一運維環境
- 方案二強調端側計算分擔負載與中樞智慧整合，適合分散式部署及有限通訊帶寬環境

---

請依據您的實際系統架構喜好與部署環境，選擇適合方案，我將依此進行進一步的任務拆解與交付規劃。若有缺少關鍵資訊，也請補充說明，有助於方案細化。
---------- TextMessage (ApplicationAgent) ----------
感謝您提出的兩套技術組合方案，針對遠距離即時海洋監控及AI影像辨識需求，我針對業務使用情境補充以下分析建議：

---

### 從使用情境角度判斷方案適用性主軸

1. **系統即時反應與持續監控需求**
   - 業務描述需24小時不間斷監控約44公里海域，且需快速標示與警示靠近關鍵區域船隻。  
   - 方案一以集中式多模型融合和多層掃描調度，能做細緻調整，同時兼顧頻率和精準度，但計算資源需求較高。  
   - 方案二以端側輕量辨識減輕中心壓力，通訊頻寬受限有利，但可能影響即時性與精準覆蓋。

2. **環境多變化下的辨識能力**
   - 海況變化大（霧氣、反光、黑白影像）且需要辨識15公尺以上船隻10艘以上。  
   - 方案一的影像增強模組及多模型融合明顯在惡劣環境下更具彈性與準確度提升空間  
   - 方案二受限於端側硬體及影像處理能力，可能較難全面因應複雜環境情況，夜間辨識效果或較弱。

3. **系統擴展性與彈性**
   - 若未來需導入雷達、AIS等多元感測器輔助判斷，以提升誤報降低及更全方位監控，方案一的模組化集中架構較易整合與擴增。  
   - 方案二則適合對分布式站點多、通訊頻寬有限且部署規模分散的場景，維運彈性較高。

---

### 綜合建議

- 若部署環境具備較佳運算資源及帶寬，且追求最高判別精準度與整體監控完整性，**方案一為優先選擇**，可精細調控多層掃描與模型融合策略，兼顧精確度與即時反應，尤其對夜間與惡劣天候辨識更具優勢。  
- 若現場硬體受限、海域監控點多且傳輸環境較差，需盡量降低傳輸負擔並分散即時處理壓力，且中心端可負擔較複雜決策演算，則可考慮**方案二**，尤其在可階段式擴展投入的情況下適用。

---

若有需聚焦的場域使用限制（如硬體配置、通訊環境、現場人力能力等），或業務對目標響應時間的更細節需求，請進一步提供，我將協助調整方案重點並優化架構建議。

您希望我再以何種方向協助？  
- 協助細化方案一或方案二應用場景流程？  
- 或是針對兩方案結合的混合方案設計建議？  
- 抑或是針對使用者操作介面與警示流程的細節想像進行引導？

期待您的回覆。
---------- TextMessage (TechnicalAgent) ----------
感謝您詳盡的補充分析與評估建議。

綜合考量貴方系統需「24小時不間斷高可靠即時監控」「遠近距船隻同時高精度識別」及「夜間與惡劣天氣條件辨識能力」，並且假設目前部署環境具備中高端運算資源與可接受一定頻寬成本（待補充實際硬體與通訊條件），

我建議聚焦 **方案一：多階段影像分級辨識系統（分層掃描+多模型融合）** 進行後續任務拆解與交付規劃。

原因如下：

- 方案一在系統即時反應及多層掃描頻率調節上彈性更大，可兼顧監控廣域與多目標分辨需求。
- 內建影像增強與注意力機制，提高環境多變化下模型辨識精度，且對夜間黑白影像辨識具備升級空間。
- 替未來多感測源融合及演算法迭代提供良好擴展性與模組化管理便利。
- 符合持續提升船隻避碰預警能力與異常事件警示的業務目標。
- 中央集中運算便於統一維運並快速更新優化演算法。

若您同意，我將根據方案一開始做系統任務拆解、交付內容建議、所需角色配置及風險管理方案，並依據方案細節協助規劃適用的驗收指標與里程碑控制。

如有其他補充硬體資源、通訊頻寬、或實際部署場域限制，也請一併提供，便於細化規劃。

期待您的決定指示。
---------- TextMessage (ApplicationAgent) ----------
感謝您詳細的補充與分析，針對您提出的情境與需求，我認為選擇方案一「多階段影像分級辨識系統（分層掃描+多模型融合）」是相當適當的策略。

以下是我對此方案的應用情境進一步想像與彙整：

- 使用者行為與工作流程：
  岸邊監控中心操作人員透過綜合影像監控平台，全天候持續監視海上情況。系統根據船隻距離自動調整掃描頻率，近距離船隻獲得密集且高解析度的掃描與辨識，遠距離船隻則以低頻率監控保持警覺。當有船隻異常靠近或出現在特定區域時，系統即時傳送視覺與警示通知，協助操作人員快速判斷情況並啟動應對流程。

- 使用時機與觸發條件：
  主要是全天候實時監控，系統會依實際感測與辨識結果動態調整運算分配。觸發條件包括監測到船隻進入關鍵海域、接近避碰距離或速度異常等狀態。

- 系統介入與回饋：
  系統整合多焦段影像來源並進行即時影像增強，利用多層模型策略強化不同環境條件的識別能力。結合空間注意力與多尺度檢測技術，提升小尺寸與遠距目標的辨識精度。辨識結果不僅回饋可視化介面即時標示，也會觸發警示音與視覺提醒，推動作業員快速反應，並支援後續資料儲存與分析以優化未來預警條件。

此方案適合貴方系統對精準度、即時度及環境多元挑戰的核心訴求，同時保障未來持續升級與擴展的彈性。

若您同意，我們可往此方案方向推進系統任務拆解與驗收規劃，並同步收集部署現場的硬體與通訊條件細節，確保設計符合現場運營限制。

請確認是否可依此進行，或有其他補充與調整方向，謝謝！

TERMINATE
---------- TextMessage (TechnicalAgent) ----------
感謝您的確認與補充說明。

針對方案一「多階段影像分級辨識系統（分層掃描+多模型融合）」，我將依下列方向進行後續任務拆解與交付規劃，並望貴方同步提供部署現場硬體與通訊環境具體資料，以利細節對齊：

---

## 一、任務拆解與依賴關係

1. **需求細化與系統設計**
   - 確認並細化多層掃描策略的距離分段與頻率規範
   - 確定所需模型版本組合（輕量模型vs深度模型）的配置邏輯與切換條件
   - 定義影像增強模組支援範圍（去霧、夜間強化等）與接口規範
   - 設計事件觸發條件及警示流程（包含預警參數設置與操作人員UI互動）

2. **前置環境建置**
   - 建置高性能運算環境（GPU集群或雲端架構）
   - 視訊採集設備整合（多焦距鏡頭、夜間感測強化設備）
   - 通訊頻寬與資料存取規劃

3. **模型開發與訓練**
   - 深度模型及輕量模型基線版本建立與測試
   - 影像增強前處理模組開發
   - 模型融合及多尺度檢測算法集成
   - 黑白影像、低光環境模型調優

4. **系統整合開發**
   - 多模型融合機制與動態調度模組實作
   - 多源影像資料流及前後端連接
   - 事件觸發系統整合與UI警示呈現開發

5. **測試驗證與調優**
   - 實場景多距離、多環境影像測試
   - 整體系統負載與延遲測試
   - 警示準確率與誤報率調整
   - 性能瓶頸及安全性測試

6. **部署與維運支援**
   - 部署系統到目標環境
   - 提供日常維運監控、模型更新及系統擴展流程

---

## 二、建議所需人力角色

- **計畫經理／技術經理**：整合協調跨團隊進度
- **系統架構師**：整體系統設計及技術評估
- **深度學習工程師**：模型訓練、調參與優化
- **影像處理工程師**：影像增強及前處理模組開發
- **後端工程師**：系統整合、服務架構及API開發
- **前端工程師**：即時視覺界面與警示呈現
- **網絡工程師**：通訊架構及資安設計
- **測試工程師**：軟硬體系統整合測試及驗收

---

## 三、潛在風險與備案建議

- **運算及頻寬負載過高**  
  備案：提升軟硬體資源，調整掃描頻率，增加端側初步過濾機制

- **惡劣環境辨識率下降**  
  備案：持續優化影像增強技術、增設多感測源（紅外、雷達）

- **系統複雜度高導致維護困難**  
  備案：模組化設計並導入自動化部署與監控

- **資料安全與隱私問題**  
  備案：加強通訊加密、身份驗證及存取權限管理

---

## 四、驗收標準與里程碑（建議範例）

- **M1 設計確認與環境建置完成**（預計1-2個月）  
  包含分層掃描策略設定、系統架構定案、環境搭建

- **M2 模型基線開發與影像增強模組測試**（預計2-3個月）  
  完成不同距離及夜間模型版本訓練與近似驗證

- **M3 系統整合與初步場景測試**（預計1-2個月）  
  多模融合、事件警示流程實測

- **M4 全面性系統測試與優化**（預計1個月）  
  包括性能、資安、穩定性測試與調校

- **M5 部署與驗收上線**（預計1個月）  
  正式啟動使用並進行後續維運準備

---

歡迎隨時提供部署硬體及通訊細節，我可協助進一步精準調整規劃細節。

期待與您共同推動項目成功落地。

TERMINATE
