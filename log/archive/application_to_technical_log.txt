---------- TextMessage (user) ----------

以下是提供你閱讀的計畫文件：
===
附件四、報告格式 
 
 
 
 
 
 
即時海洋觀測平台開發暨AI影像數據應用服
務主題式研發計畫 
 
期末報告1 
 
即時海洋觀測平台開發暨 AI 影像數據應用服務主題式研發
計畫 
 
 
 
 
期末報告 
 
 
 
 
 
 
 
 
計畫主持人：林家瑜 助理教授     
研究機構  ：國立中央大學資訊工程系 
執行期間  ：113/05/01-114/04/30（請填寫合作研究計畫執行期間） 
 
(以上資料請務必以中文繕打清楚) 
 
中華民國 114 年 01 月31 日2 
 
計畫摘要 
    本研究致力於開發即時海洋觀測平台，並應用 AI 影像數據分析來提升遠距離船
隻辨識的準確性與效率。現行船舶識別技術主要依賴人眼辨識，然而在 3 公里以上距
離，由於海面反射、霧氣與光線變化等環境因素的影響，傳統方法的準確率明顯下
降。本計畫旨在透過深度學習物件辨識技術，建立一套高效能的「即時遠距離多船隻
辨識系統」，能夠在 3 公里外準確偵測船隻，並可同時識別 10 艘以上長度超過 15 公
尺的船隻。 
    本研究參考 YOLOv5s 與YOLOv8 物件辨識模型，針對海洋環境特性進行改良，
並利用多種公開及自建數據集如 Singapore Maritime Dataset、ABOships-PLUS、高雄
港實測數據進行模型訓練與驗證。透過影像增強、特徵提取優化與多尺度檢測技術，
我們顯著提升了模型在複雜海洋場景中的識別能力。 
    目前的實驗結果顯示，我們的系統在新加坡與高雄測試數據集中的部分影像皆展
現超過 90%的識別準確度，並成功達成： 
⚫ 可偵測移動速度超過 15 節的船隻 
⚫ 在 5 公里範圍內實現高效識別 
⚫ 可即時處理 640×480 解析度影像，幀率≥10FPS 
⚫ 具備船隻避碰預警功能 
    未來將進一步提升船隻辨識與船隻避碰預測的準確度，並擴充模型至夜間與黑白
影像辨識，以提升即時監測與應用能力。此技術的發展對於海洋交通安全監控、智慧
港口管理與海上救援應用將具備重要價值。 
 
關鍵字：即時海洋觀測、AI 影像辨識、深度學習、YOLOv5、YOLOv8、多尺度檢
測、船隻辨識、避碰預警、智慧港口、遠距離監測3 
 
目錄 
計畫摘要 ....................................................................................................................... 2 
一、計畫動機與目的 ................................................................................................... 4 
1. 動機 ..................................................................................................................... 4 
2. 目的 ..................................................................................................................... 4 
二、文獻探討 ............................................................................................................... 5 
三、研究方法 ............................................................................................................... 8 
1. 資料收集 ............................................................................................................. 8 
2. 資料前處理 ....................................................................................................... 10 
3. 船隻辨識模型 ................................................................................................... 10 
四、實驗結果 ............................................................................................................. 12 
五、結論 ..................................................................................................................... 18 
六、參考文獻 ............................................................................................................. 194 
 
一、 計畫動機與目的 
1. 動機 
在海洋環境中，對於船隻的及時且準確的辨識是保障航行安全和有效執
行海上作業的關鍵。但目前大部分的船隻都是人工辨識，而人眼無法同時看
超過 10 艘船隻進行辨識，因此需要 AI 輔助。現有的 AI 對距離近且較大的
船或是俯視角的衛星雷達成像的船有不錯的辨識能力，但若是在第一人稱且
距離超過 3 公里的情況下，可能會受到環境條件如海面反射、霧氣、光線變
化等的影響，導致偵測的準確度降低，且現有開放資料集大多是較大的船隻
，3 公里以上的船隻資料集較少，不利於訓練模型。 
2. 目的 
基於上述的問題，本計畫欲設計一套「即時遠距離多船隻辨識系統」 ，
此系統可以有效辨識 3 公里遠的船隻，且一次可辨識長度15 公尺的船隻 10
艘以上。5 
 
二、 文獻探討 
現在有一些研究採用物件辨識模型進行船隻辨識，在 Automatic Ship 
Object Detection Model Based on YOLOv4 with Transformer Mechanism in 
Remote Sensing Images 論文中提出了名為 Auto-T-YOLO 的架構[1]，用於解
決現有物件偵測模型從輸入中提取特徵未能充分考慮全局特徵，且無法根據
輸入的特徵自動調整，導致偵測準確度下降的問題。 
    Auto-T-YOLO 的模型架構分為三個部分 ，Preattention, Attention 和
Prediction。輸入影像大小為608×608×3，Preattention 和 Attention 的設計
模仿了人類視覺系統中從大的全局特徵開始，再逐步轉向局部細節的兩階段
注意機制。 
    在 Preattention 架構中，輸入的圖像會根據其中是否包含𝑠𝑖𝑧𝑒 > 32×32
的物件分為 A 子資料集及 B 子資料集，並在之後的 Attention 架構中根據其
所在的子資料集做不同的影像處理。 
    Attention 架構主要參考了 YOLOv4 的 Backbone 及 Neck 架構來設計，
A 子資料集及 B 子資料集在 Backbone 中的處理流程相同，都是利用一層
CBM 進行卷積運算以及五層 CSPDarknet53 進行殘差及卷積運算來提取特
徵同時防止梯度消失，由於YOLOv4 的 CBL 架構基於卷積運算，而卷積運
算受限於 Kernel 大小，因此無法很好地提取全局特徵，所以 A 子資料集在
Neck 中 SPP 架構輸出之後使用了 Transformer 中的 Multi-head Self-Attention
架構取代了 CBL，稱為 SPP-MHSA，其中 Multi-head Self-Attention 使用了
四個 head。Neck 的後半段與 YOLOv4 相似，使用了 FPN 做 Upsampling、
PANet 做 downsampling 以提升多尺度目標檢測的能力。 
    Prediction 架構用來對 PANet 輸出做多尺度目標檢測，損失函數使用了
CIoU 以及 DIoU-NMS。模型準確度在 SSDD 資料集、近海及遠海場景上分
別達到了 96.3%、91.78%以及 98.33%。 
 在另一篇論文 A streamlined approach for intelligent ship object detection 
using EL -YOLO algorithm 中提出了 EL-YOLO 的架構[2]，用於提升目標檢
測在具有波浪、反射等複雜海洋環境中對於小物體的檢測精度，其輕量化的
網路架構在硬體運算資源有限的智慧船舶、無人船舶等領域也有良好的適應
性。 
EL-YOLO 的架構基於 YOLOv8 進行改進，輸入影像大小為640×640
，包含了Backbone、Neck、Head 三個部分，Backbone 中與 YOLOv8 相同，
使用了一層CBS 進行卷積運算和四層CBS 加C2f 進行殘差及卷積運算來提
取特徵同時防止梯度消失。 
在 Neck 部分，因為傳統的 FPN 和 PAN 架構在面對因反射或波浪等背
景干擾因素導致特徵信息不足的目標時，容易在多層卷積過程中丟失信息，
導致檢測精度下降，所以作者提出了 SMFN (short multi‑fuse neck )架構，在6 
 
YOLOv8 的基礎上，額外加入了較大尺寸的兩個輸入 (320×320和160×
160)、一個 Upsampling 層(160×160) 、一個 Fusion 加 C2f 層(160×160)
、一個卷積層(80×80)及一個輸出(160×160)到 FPN 和 PAN 架構中，以
捕捉更多的特徵信息，並提高對小型目標的檢測能力。 
Head 架構因為 Neck 的 SMFN 架構而多了一個160×160的輸出，相較
原本只有三個輸出的 YOLOv8 多了一個。在損失函數的部分，因為傳統的
CIoU 在處理特徵信息不足的目標時 往往會過度懲罰， 因此作者提出了
AWIoU(adequate wise IoU )，通過動態分配不同權重給不同特徵信息量的目
標來調整懲罰，避免目標因受背景干擾而造成模型誤判。 
在模型訓練完後，為了使其網路架構輕量化以應用於硬體運算資源有限
的智慧船舶 、 無人船舶等領域，作者提出了GDFP(Greedy-driven filter pruning)
，首先計算每一層中所有 filter 的 L1 範數以得到其重要性分數，並將分數最
低的 filter 刪除，在刪除後對模型重新評估，並重複以上操作直到得到理想
的模型，最後重新訓練該模型，以得到將模型輕量化的效果。 
此模型在 SeaShips 資料集上的mAP0.5 為 0.988， mAP0.5:0.95 為 0.778
，在 ABOships 資料集上的mAP0.5 為 0.672，mAP0.5:0.95 為 0.348。 
另一篇論文  YOLOSeaShip: a lightweight model for real -time ship 
detection [3]中提出了 YOLOSeaShip 的架構，用於解決海洋環境中目標檢測
的準確性與硬體資源需求之間的矛盾，特別是在實時性要求較高且硬體運算
能力有限的應用場景如邊境監控、智慧海洋交通中。 
    YOLOSeaShip 的架構基於 YOLOv7-tiny 進行改進，包含了 Input、
Backbone 和 Head 三個主要部分。 
Input 架構上採用了 Mosaic 數據增強技術，通過隨機裁剪並拼接多張影
像，增加數據的多樣性與小目標樣本數。此外，使用自適應圖片縮放技術來
處理不同尺度的目標，並結合K-means 分群方法優化錨框的尺寸設定，避免
人工設計過程中的偏差。 
Backbone 的部分，YOLOSeaShip 引入了 PConv（Partial Convolution ）
架構替代了原本的卷積操作。該架構僅對部分通道進行計算，從而有效減少
了參數量並提升計算效率。此外，每層池化操作後 還加入了 PfAAM（
Parameter-Free Average Attention Module）。PfAAM 基於空間和通道的平均值
計算注意力權重，不增加模型參數的情況下提升了模型對關鍵區域的定位能
力。 
Head 部分，模型加入了SPPCSPC_tiny 架構，通過多尺度（5×5、9×9 和
13×13）最大池化層結合本地與全局特徵，增強對不同大小目標的檢測能力
。模型的最終輸出包含三個不同大小的特徵圖，用於處理不同尺度的目標檢
測。 
在損失函數設計上，作者提出了 Focal EIOU（Focal and Efficient IOU）
損失函數，結合了 EIOU 和 Focal Loss 的優點。EIOU 損失通過細化目標框7 
 
的寬高比差異來提升回歸精度，而Focal Loss 則用於平衡正負樣本比例，讓
模型更關注高質量的預測框。 
為了評估模型性能，作者在 SeaShips(7000)資料集上進行實驗，訓練時
採用 Adam 優化器，學習率為 0.001，訓練 300 個 epoch。結果顯示，
YOLOSeaShip 的 mAP 達到 0.976，檢測速度達到119.84 FPS，相較YOLOv7-
tiny 模型，參數量減少至 4.78M，同時大幅提升了檢測速度和資源效率。 
在論文 An Improved YOLO v4 Algorithm-based Object Detection Method 
for Maritime Vessels[4]中提出了基於改進 YOLOv4 的船舶和泊位檢測方法，
用於提升目標檢測在複雜海洋環境下的準確性，特別是對港口泊位的檢測效
果進行了優化。該方法通過改良 錨框的生成方式和數據集構建，改善了
YOLOv4 在海洋場景中的檢測性能，適用於智能船舶的安全輔助系統。 
改進的 YOLOv4 架構包含了 Backbone、Neck 和 Head 三個部分。
Backbone 部分採用了 CSPDarknet53 作為特徵提取模塊，通過 ResUnit 組件
進行深度卷積操作，增強特徵提取能力。Neck 部分引入了 SPP 層和 PANet
層，其中 SPP 層通過多尺度池化操作擴展感受野，而 PANet 則將底層的定
位信息與高層的語義信息結合，提高多尺度目標的檢測能力。Head 部分則
採用了 YOLOv3 的檢測頭，輸出不同尺度的檢測結果。 
針對泊位檢測效果不足的問題，作者採用了 K-means 演算法重新生成
錨框。在原始 YOLOv4 模型中，錨框基於 MS COCO 和 VOC 數據集生成，
但這些框與船舶與泊位數據集中的物體尺寸不匹配，影響了檢測精度。改進
後的 K-means 演算法針對數據集進行分群，生成了9 組適配的錨框，有效提
升了檢測效率和準確性。 
此外，作者構建了一個包含船舶和泊位的專用數據集，通過智能實驗船
上的攝像頭在港口拍攝的實景影像生成，並結合公開數據集如MS COCO 和
VOC 補充正樣本數量。最終數據集中包含6634 張標註影像，其中70%用於
訓練，30%用於測試。標註信息包括物體類別及邊界框的坐標和尺寸。 
實驗結果表明，改進的YOLOv4 模型在 mAP 和 F1-score 方面分別提升
了 2.79%和 0.80%，相較於原始YOLOv4 達到了更高的檢測精度與穩定性。
具體指標顯示，改進後的YOLOv4 模型在 mAP 上達到 83.98%，F1-score 達
到 86.18%，而 YOLOv4_tiny 僅達到 54.45%的 mAP 和 54.82%的 F1-score，
說明輕量化模型雖然具有更快的檢測速度，但在檢測準確性上仍有進步空間
。8 
 
三、 研究方法 
1. 資料收集 
    資料集使用 Singapore Maritime Dataset 的 Visible On-Shore 資料集[5, 6]加上
ABOships-PLUS[7]以及手動標註公司提供的高雄資料集。 
    Visible On-Shore 資料集為網路上的公開資料集，包含 38 部長度小於 33 秒
的全彩影片，屬於中、遠港船隻影像資料集，其中含有 7 個船隻類別，此資料集
特徵為船隻背景為海面、天空等較乾淨的畫面。 
 
表一、Visible On-Shore 資料集類別 
Lable Data   
Ferry 
 
SailsBoat 
 
PatrolBoat 
 
MerchantShip 
 
FishingBoat 
 
Tugboat 
 
BargeShip9 
 
    ABOships-PLUS 資料集為網路上的公開資料集，ABOships 資料集的改進版
本，包含 9880 張影像，33227 個標註，屬於近港船隻影像資料集，其中含有 3 個
船隻類別，此資料集特徵為船隻背景包含陸地建築物等容易誤判的畫面。 
 
表二、ABOships-PLUS 資料集類別 
Lable Data   
powerboat 
 
sailboat 
 
ship 
 
  
 
    高雄資料集為高雄實地拍攝的非公開資料集，包含 504 部長度小於 10 秒的
影片，屬於近、中港船隻影像資料集，其中含有 4 個船隻類別，此資料集特徵為
船隻背景包含陸地建築物等容易誤判的畫面。 
 
表三、高雄資料集類別 
Lable Data   
PatrolBoat 
 
Yacht 
 
MerchantShip 
 
Tugboat10 
 
2. 資料前處理 
    
 
圖一、資料前處理流程圖。 
    如圖一所示，首先會將資料集處理成只包含 images 和 labels 資料夾的
形式，images 和 labels 中的檔案是兩兩對應的，並且labels 內的標註檔格式
為 yolo 格式。再來會刪除有問題的標註檔及圖片，例如標註檔為空、包含
不存在類別、格式錯誤、圖片模糊或者有破圖情況等。第三步是合併資料集
，這步是可選的，主要會將數個資料集合併，並記錄合併後的類別名稱。第
四步是篩選類別，這步同樣為可選，主要會將需要的類別保留，不需要的剔
除。接著因為同一圖片中可能包含數量不等的不同類別物件，因此在切分訓
練集和驗證集時，容易造成不同類別的訓練驗證比例不同的情況，所以第五
步會將包含 n 個物件的標註檔拆分成 n 份，每一份只包含1 個原始標註檔的
物件，彼此不重複，且與其對應的圖片同樣複製 n 份，以確保每個類別都按
照正確比例進行切分，最後一步為按照自訂比例切分訓練集及驗證集。 
3. 船隻辨識模型 
    我們使用了最新的YOLOv11[8]模型來建置船隻辨識模型 ，YOLOv11 基
於 PyTorch 架構開發，可在一次前向傳播中同時定位及辨識物件，具有速度
快、所需資源少、開發便利及多尺度檢測等優點。圖二為 YOLOv11 架構圖
，首先輸入的圖片為 640x640x3 的 RGB 圖像，接著 YOLOv11 可以根據此
架構分為三個部分，Backbone, Neck 以及 Head。11 
 
 
圖二、YOLOv11 架構圖[9] 
 
Backbone 如圖二左側所示，主要目的是從輸入影像中提取有用的特徵
資訊。在 YOLOv11 中，Backbone 採用了 C3K2 結構，為升級版的 C3，透
過更小的卷積核與路徑切分技術，在保持模型表現的前提下大幅降低了計算
成本。每個C3K2 中包含兩條路徑，一條直接進行卷積處理，另一條則經過
多個 Bottleneck 區塊進行深度特徵學習，最後進行特徵融合。這種結構不僅
有效防止梯度消失問題，也能提升網路的收斂速度與特徵提取效率。 
Neck 部分如圖二中間所示，結合了 SPFF(Spatial Pyramid Pooling Fast)
架構和多層 C2PSA 區塊。SPFF 架構是 SPP 的高速版本，透過不同尺度的
最大池化操作將特徵圖進行多尺度融合，再透過 Concatenation 將資訊彙整
，幫助模型更好地識別大小不一的船隻。此外，Neck 中的 C2PSA 架構引入
空間注意力機制(Spatial Attention)，能讓模型聚焦在關鍵影像區域，特別是
對小型或部分遮蔽的物件表現更佳。這部分的架構延續了FPN 與 PAN 的優
點，強化了從深層到淺層、再回到深層的資訊傳遞，進一步提升辨識與定位
的能力。 
Head 如圖二右側所示，負責進行最後的物件偵測任務。YOLOv11 採用
多尺度輸出策略，透過多層輸出特徵圖分別負責不同大小目標的偵測。
YOLOv11 會利用不同尺寸與比例的錨框(Anchor Boxes)來涵蓋不同形狀的
目標，並透過 Non-Maximum Suppression(NMS)過濾重疊的邊界框，保留最
具代表性的偵測結果。12 
 
四、 實驗結果 
    我們使用 YOLOv11l 模型，訓練集使用 ABOships-PLUS，測試集使用
高雄資料集，實驗環境為 64-bit Windows 11、Intel i5-13400F 2.5GHz CPU、
32GB RAM 及 NVIDIA RTX 4070 GPU。除了測試模型在原先新加坡資料集
的效果外，實驗還在高雄資料集上針對船隻速度、距離以及避碰能力做檢測
，因實際場域在白天時使用的是彩色影像，黑夜時使用的是黑白影像當作模
型輸入，所以實驗也同時測試了模型對彩色和黑白影像的辨識能力，檢測方
式為當船隻出現的 3 秒內有辨識到至少 1 次則辨識成功，反之辨識失敗，同
一部影片中辨識成功的數量與真實船隻的數量比值即為檢測率。實驗時我們
同時比較了 1 秒內有辨識到船隻、2 秒內有辨識到船隻的檢測率以更精確地
測量模型的辨識能力，以下為檢測效果。 
 
表四、實驗結果檢測率 
測試影片 1 秒內檢測率 2 秒內檢測率 3 秒內檢測率 
彩色影片 output_005 1.0 1.0 1.0 
彩色影片 output_041 1.0 1.0 1.0 
彩色影片 output_080 1.0 1.0 1.0 
彩色影片 output_097 1.0 1.0 1.0 
黑白影片 output_199 0.67 1.0 1.0 
黑白影片 output_202 0.67 1.0 1.0 
黑白影片 output_276 1.0 1.0 1.0 
黑白影片 output_290 0.75 0.75 1.0 
 
    如上表所示，原要求的 3 秒內檢測率全部達標，都為 1.0，2 秒與 1 秒
內的檢測率也大部分都是 1.0，其中判錯的情況都是在船隻彼此重疊時或影
像較模糊時發生，一般的應用情況幾乎不會有問題。13 
 
 
圖三、彩色影片 output_005 第一秒影像 
    彩色影片 output_005 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。 
 
圖四、彩色影片 output_041 第一秒影像 
    彩色影片 output_041 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。14 
 
 
圖五、彩色影片 output_080 第一秒影像 
    彩色影片 output_080 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。 
 
圖六、彩色影片 output_097 第一秒影像 
    彩色影片 output_097 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。15 
 
 
圖七、黑白影片 output_199 第二秒影像 
    彩色影片 output_199 的第二秒影像如上圖所示，圖中模型成功辨識出所
有船隻，2~3 秒內的檢測率都為 1.0。 
 
圖八、黑白影片 output_202 第二秒影像 
    彩色影片 output_202 的第二秒影像如上圖所示，圖中模型成功辨識出所
有船隻，2~3 秒內的檢測率都為 1.0。16 
 
 
圖九、黑白影片 output_276 第一秒影像 
    彩色影片 output_276 的第一秒影像如上圖所示，圖中模型成功辨識出所
有船隻，1~3 秒內的檢測率都為 1.0。 
 
圖十、黑白影片 output_290 第三秒影像 
    彩色影片 output_290 的第三秒影像如上圖所示，圖中模型成功辨識出所
有船隻，3 秒內的檢測率都為 1.0。17 
 
表五、KPI 完成表 
項目 達成狀況 
1.建立 AI 分析海事觀測演算法邏輯架構 已達成 
2.完成演算法建立，並可在本島沿海 3 公里海域內環境進行 已達成 
3.演算結果可達以下規格標準: 
通訊傳輸速率>250kbps 以上 已達成 
影像解析度> 640x480 已達成 
影像幀速率≧ 10FPS 已達成 
船隻移動速度≧15 節(30km/hr) 已達成 
船隻辨識數量≧ 10 已達成 
船隻辨識大小> 15m 已達成 
船隻辨識距離≧5km 已達成 
船隻避碰距離≧3.7km 已達成 
船隻辨識準確度≧90% 已達成 
 
KPI 完成表如上所示，全部項目都已完成。18 
 
五、 結論 
本計畫目的為設計出可以有效辨識 3 公里遠的船隻，且一次可辨識長度 15
公尺的船隻 10 艘以上的模型，根據目前的實驗結果，模型在船隻距離、速度、
目標數量以及避碰能力上都有達到目標，並且模型在彩色及黑白影片上的表現也
能夠符合實際應用場域需求。19 
 
六、 參考文獻 
[1] Sun, Bowen, et al. "Automatic ship object detection model based on YOLOv4 
with transformer mechanism in remote sensing images." Applied Sciences vol. 
13. No. 4, p. 2488, 2023. 
[2] Yang, Defu, et al. "A streamlined approach for intelligent ship object detection 
using EL-YOLO algorithm." Scientific Reports, vol. 14. No.1, p.15254, 2024. 
[3] Jiang, X., Cai, J., & Wang, B. (2024). YOLOSeaShip: a lightweight model for 
real-time ship detection. European Journal of Remote Sensing, 57(1). 
https://doi.org/10.1080/22797254.2024.2307613 
[4] He, Guowen & Wang, Wenlong & Shi, Bowen & Liu, Shijie & Xiang, Hui & 
Wang, Xiaoyuan. (2022). An Improved YOLO v4 Algorithm-based Object 
Detection Method for Maritime Vessels. International Journal of Science and 
Engineering Applications. 11. 50-55. 10.7753/IJSEA1104.1001. 
[5] D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabaly, and C. Quek, “Video 
Processing from Electro-optical Sensors for Object Detection and Tracking in 
Maritime Environment: A Survey,” IEEE Transactions on Intelligent 
Transportation Systems (IEEE), 18 (8), 1993 - 2016, 2017. 
[6] D. K. Prasad, “Singapore Maritime Dataset, Visible On-Shore dataset,” Available 
Online: https://sites.google.com/site/dilipprasad/home/singapore-maritime-
dataset 
[7] Winsten Jesper, Iancu Bogdan, Soloviev Valentin, & Lilius Johan. (2024). 
ABOships-PLUS [Data set]. Zenodo. https://doi.org/10.5281/zenodo.10469672 
[8] Glenn Jocher and Jing Qiu, “Ultralytics YOLO11,” Available Online: 
https://github.com/ultralytics/ultralytics 
[9] Medium, “YOLOv11 Architecture Explained: Next-Level Object Detection with 
Enhanced Speed and Accuracy,” Available Online: https://medium.com/@nikhil-
rao-20/yolov11-explained-next-level-object-detection-with-enhanced-speed-and-
accuracy-2dbe2d376f71
===
==========
### 使用情境說明

**使用者背景與目標**  
岸邊監控人員透過固定攝影設備監控遠海範圍（24海里約44公里）內的船隻動態，目標是即時辨識多艘中大型船隻（以15米以上為主），即時掌握船隻位置與行動狀態，並在未來能結合避碰預警與船速判斷等輔助功能，提升海上交通安全與管理效率。

**使用時機與場景**  
系統需全天候運作，涵蓋多變的海況環境（包含晴朗、霧氣等），提供至少約10框/秒的影像更新頻率以確保資訊即時性。監控設備部署在岸邊固定監控點，無現有感測器輔助，但可能未來配合更多硬體整合。

**操作流程**  
監控人員透過系統查看遠距離監控畫面，系統自動辨識範圍內多艘船隻，辨識結果以視覺化方式呈現，並提供船隻種類與位置資訊。當系統偵測到異常狀況或避碰風險時，可發出預警，提示操作人員進行進一步判斷和應對。

**預期結果與效益**  
- 維持高水準的辨識準確度（接近現有3公里方案90%以上）  
- 即時反映多船隻動態，提升監控效率  
- 利用AI輔助減少人工辨識負擔  
- 強化海上交通安全與風險管理

---

### 主要技術挑戰與限制

1. **影像品質與設備需求**  
遠距離（24海里）監控對攝影設備提出了極高要求，須配備高倍光學變焦鏡頭及高解析度攝影機，以克服距離衰減及氣象因素（霧氣、海面反光等）導致的影像模糊。

2. **AI模型適應性**  
現有模型主要針對約3公里範圍設計與訓練，面對更遠距離及小目標辨識，需要針對遠距離特徵調整或進行重新訓練，強化對低解析度及複雜背景的辨識能力。

3. **數據傳輸與運算負擔**  
高解析度影像和高幀率更新帶來大量數據流量，對網路帶寬和後端運算能力構成挑戰，需提升數據壓縮、傳輸和即時處理能力。

4. **環境適應性**  
霧氣、光線變化及海面波浪反射等海洋環境因素，在極遠距離監控下更明顯，可能導致辨識準確度下降。需配合多感測器融合（如雷達、熱成像）或開發針對特殊場景的AI模組來補強。

5. **系統整體佈建與成本考量**  
高性能監控設備與大數據傳輸需考慮成本與部署可行性，可能須分階段導入、搭配現有硬體更新，以平衡性能與投資。

---------- TextMessage (TechnicalAgent) ----------
感謝提供詳盡計畫報告與使用情境說明。以下針對「24海里（約44公里）遠距離岸邊即時多船隻辨識系統」提供可行技術解法規劃與專案交付建議。

---

## 第一階段：技術解法規劃

### 技術方案一：基於擴展式多階段影像處理及深度學習結合多感測器融合

#### 適用條件
- 需突破現有3公里遠距離限制，提升遠距離影像品質及細節。
- 可配合高倍光學變焦攝影機、熱成像及雷達感測器。
- 適合大型專案、資源充裕、要求高精度與高可靠性場景。

#### 主要模組與流程
1. **影像前端裝置**
   - 高倍光學變焦鏡頭高速攝影機（支援1080p以上解析度）。
   - 配備熱成像攝影與雷達（毫米波）感測器進行多源資料補強。
2. **多感測器資料融合模組**
   - 將可見光影像與熱影像、雷達訊號進行融合，減低霧氣、光線及反射干擾對辨識之影響。
3. **多階段影像增強處理**
   - 包含去霧演算法（如基於深度學習的dehazing）、超解析度增強、動態對比度調整。
   - 特別針對小目標與遠距離目標進行影像細節補強。
4. **客製化深度學習模型推理**
   - 參考YOLOv11結構，訓練以遠距離多尺度小目標為主的專用模型。
   - 模型融合注意力機制及空間特徵增強（參考改良版C3K2結構與Spatial Attention）。
5. **即時物件偵測與軌跡預估**
   - 多目標追蹤（MOT）與船速/航向估算演算。
6. **避碰預警與行為分析**
   - 基於動態船隻位置推論避碰風險，智能發出警報。
7. **可視化儀表板**
   - 實時地圖疊加監控界面，顯示辨識結果與船隻狀況。

#### 擴展性與彈性
- 硬體可彈性增加感測器（如未來雷達、AIS整合）。
- AI模型可持續迭代並支援多場景訓練數據更新。
- 模組化設計，可針對不同環境需求調整影像處理與融合算法。

#### 潛在風險與限制
- 多感測器成本與維護高，前端設備投資昂貴。
- 多元資料融合複雜，系統整合挑戰大。
- 大量數據傳輸與即時處理需高性能運算與通訊支援。
- 若置於惡劣天候，依然可能遭遇影像解析度不足、資料不完整風險。

---

### 技術方案二：基於端側輕量模型與分層雲端運算架構

#### 適用條件
- 受限設備成本與現場計算資源情況。
- 重點在降低資料流量與分散運算負擔。
- 適用於分散多監控點的彈性佈建。

#### 主要模組與流程
1. **前端端側輕量模型推理**
   - 以輕量版模型（如改良YOLOSeaShip或EL-YOLO）部署於岸邊攝影點本地運算裝置（Edge TPU/GPU）。
   - 進行初步船隻偵測與分析，輸出摘要目標資訊（位置座標、類別、尺寸）。
2. **輕量模型定期更新**
   - 模型定期由中央訓練平台推送更新以適應遠距離與新場景。
3. **資料抽象化與壓縮傳輸**
   - 僅發送偵測結果（Meta data）或關鍵影像段，減少頻寬使用。
4. **雲端多監控點資料整合**
   - 集中接收各端節點目標資料，進行多目標追蹤與跨監控點聯合分析。
5. **高層避碰預警與行為分析雲端服務**
   - 基於全局視角進行避碰判斷、風險分析、異常行為偵測。
6. **可視化使用介面**
   - 雲端儀表板提供整合監控畫面與警示推播。

#### 擴展性與彈性
- 利用端側運算減少核心網路壓力，適合廣泛部署。
- 可逐步增加端側算力或升級輕量模型，延伸系統能力。
- 雲端集中式管理與更新，方便迭代新功能。

#### 潛在風險與限制
- 輕量模型辨識精度相較大型模型略低（尤其在超遠距離及複雜背景下）。
- 端側硬體要求仍需能支援10FPS以上推理，邊界條件需測試確認。
- 資料同步延遲可能影響即時性。
- 對通訊品質依賴較高，斷線時系統效能下降。

---

### 技術方案三（備選）：基於影像特徵預處理與自適應超解析度強化＋單一強化深度學習模型

#### 適用條件
- 無法增加多感測器，只能透過單一攝影機提升遠距離細節。
- 聚焦先進影像處理技術強化原始影像訊號。

#### 主要模組與流程
1. **前端採用自適應超解析度影像增強算法**
   - 基於近代深度學習超解析度(SR)模型，動態增強小目標細節。
2. **動態環境自適應影像去噪／去霧算法**
   - 依海況調整影像處理參數，改善霧效與反光干擾。
3. **單一改良深度學習模型 (基於YOLOv11)**
   - 使用經強化影像的結果進行偵測，並利用空間注意力機制提升小目標辨識。
4. **後端雲端運算處理與避碰預警**
   - 集中處理多來源資料，支持大量運算資源。

#### 擴展性與彈性
- 不需多感測器，降低部署複雜度與成本。
- 可結合未來更多影像資料提升模型效能。

#### 潛在風險與限制
- 超解析度與影像增強演算法即時性挑戰大（可能產生延遲）。
- 對惡劣天氣效果改善有限。
- 單一感測器容易受場景限制。

---

## 第二階段：任務拆解與交付建議

### 選定方案建議
依據使用情境要求全天候24海里遠距離、即時性高、且預期具有避碰預警功能，建議採用**方案一（多感測器融合＋多階段影像增強＋改良深度模型）**以取得最佳辨識效果與長遠擴展性。

---

### 任務步驟拆解與依賴關係

| 項次 | 任務名稱                             | 依賴任務         | 主要輸出                            |
|-------|------------------------------------|------------------|----------------------------------|
| 1     | 詳細需求確認與設備選型              | -                | 設備規格與部署方案                |
| 2     | 視覺感測硬體採購與部署              | 1                | 高倍光學變焦攝影機、熱像儀、雷達  |
| 3     | 多感測器資料同步及融合技術開發       | 2                | 資料融合模組初版                  |
| 4     | 海況影像前處理演算法設計            | 3                | 去霧、超解析度、對比度優化演算法  |
| 5     | 影像資料集收集與標注擴充            | 1                | 遠距離多場景豐富資料集            |
| 6     | YOLOv11模型改良與針對遠距離訓練     | 5                | 遠距離專用辨識模型版本            |
| 7     | 模型與多感測器融合算法整合測試       | 3,4,6            | 模型融合一體化功能                |
| 8     | 多目標追蹤與避碰預警系統開發         | 7                | 避碰預警模組                     |
| 9     | 系統端到端測試與效能優化             | 8                | 穩定有效的整合系統                |
| 10    | 使用者介面與視覺化工具開發           | -                | 監控儀表板與警示系統              |
| 11    | 現場部署、上線與訓練操作人員         | 2,9,10           | 系統正式上線及使用培訓            |
| 12    | 後續系統維護與模型迭代               | 11               | 持續迭代能力與優化方案            |

---

### 建議人力結構

| 職能          | 人力建議           | 主要任務                           |
|---------------|--------------------|----------------------------------|
| 系統架構師    | 1                  | 整合系統架構設計與規劃             |
| 軟體工程師    | 2-3                | 演算法實現、多感測器融合、系統整合   |
| AI工程師      | 2-3                | 模型訓練、改良，資料處理           |
| 前端工程師    | 1-2                | 使用者介面及視覺化儀表板開發        |
| 資料工程師    | 1-2                | 資料收集、標註、預處理             |
| 硬體工程師    | 1-2                | 感測器部署、設備維護               |
| 測試工程師    | 1                  | 整合測試、性能驗證                |
| 專案經理      | 1                  | 專案管理與協調                    |

---

### 風險與備案

| 風險                    | 可能影響                             | 備案與緩解措施                              |
|-------------------------|------------------------------------|--------------------------------------------|
| 感測器成本與部署困難    | 延期，超出預算                      | 第一階段先小規模部署，分期採購與調整規劃。  |
| 多感測器資料同步技術挑戰| 融合效果不好，影像資料不一致        | 引入模擬測試環境與分模組驗證確保品質。        |
| 遠距離小目標辨識準確率低| 影響系統整體辨識效果                | 強化多數據擴充，模型多次迭代及領域調校。      |
| 網路傳輸瓶頸            | 延時影響即時性                      | 使用邊緣運算結合壓縮技術，優化資料流。       |
| 天候因素嚴重影響影像質量| 錯誤警報或漏報                      | 配備熱成像與雷達，多源資料降低誤檢。          |

---

### 驗收指標與里程碑

| 里程碑                       | 目標                                     | 驗收指標                                  |
|------------------------------|------------------------------------------|-------------------------------------------|
| 設備完成部署與資料採集       | 全場點部署完成，完成原始影像與感測資料收集  | 設備正常運行，資料收集完整性達95%以上       |
| 多感測器融合模組完成測試     | 成功融合多感測器資料，達到基準準確度         | 加強辨識準確率提升10%以上，降低誤檢率5%以上  |
| 影像增強與AI模型整合完成     | 模型測試符合至少90%遠距離辨識準確率要求     | 所有測試影像辨識率≥90%，FPS≥10              |
| 避碰預警模組功能上線         | 成功即時發出避碰警示，反應時間＜500ms       | 測試避碰場景下警報命中率≥95%，漏報＜5%        |
| 全系統現場試運行             | 連續運行1個月無重大故障，準確率、穩定性保持目標 | 99%系統正常運作時間，使用者滿意度80%以上      |

---

以上方案與執行計劃，將可有效支援遠距離即時多船隻辨識，並具備良好擴展性以因應未來功能擴充與多感測器融合需求。

  
TERMINATE